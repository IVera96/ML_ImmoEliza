{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Price Prediction using Neural Networks\n",
    "\n",
    "In this notebook, we build a machine learning model using a neural network to predict real estate prices. The dataset includes various features such as living area, province, salary, and average price per square meter. The goal is to predict property prices based on historical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "This section contains all necessary imports to keep the code organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine learning and deep learning libraries\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Custom functions\n",
    "from functions import adjust_row, remove_outliers, accuracy, smape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "This section loads the dataset, removes outliers, and applies necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_path = (\n",
    "    r\"./Data/data_clean.csv\"\n",
    ")\n",
    "df = pd.read_csv(original_data_path)\n",
    "df = pd.read_csv(\"Data\").drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Remove outliers\n",
    "df = remove_outliers(df)\n",
    "\n",
    "# Create synthetic data\n",
    "df_synth = df.copy()\n",
    "df_synth[[\"Living_Area\", \"Price\"]] = df_synth.apply(adjust_row, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting\n",
    "The dataset is divided into training (80%) and test (20%) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(df)[: int(0.8 * df.shape[0])]\n",
    "test = np.array(df.copy())[int(0.8 * df.shape[0]) :]\n",
    "\n",
    "synthetic_data = np.array(df_synth)[: int(0.8 * df.shape[0])]\n",
    "train = np.concatenate((train, synthetic_data), axis=0)\n",
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Normalization\n",
    "The data is scaled using MinMaxScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scaled using the mean and standard deviation of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "X_train, y_train = train_scaled[:, 1:], train_scaled[:, 0]\n",
    "X_test, y_test = test_scaled[:, 1:], test_scaled[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Neural Network Model\n",
    "A sequential model is built with hyperparameter tuning using Keras Tuner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int(\"units_layer_1\", 800, 1000, 50), activation=hp.Choice(\"activation_layer_1\", [\"relu\", \"tanh\", \"sigmoid\"]), input_dim=X_train.shape[1]))\n",
    "    \n",
    "    if hp.Boolean(\"add_layer_2\"):\n",
    "        model.add(Dense(hp.Int(\"units_layer_2\", 600, 800, 50), activation=hp.Choice(\"activation_layer_2\", [\"relu\", \"tanh\", \"sigmoid\"])))\n",
    "    \n",
    "    model.add(Dense(hp.Int(\"units_layer_3\", 400, 600, 50), activation=hp.Choice(\"activation_layer_3\", [\"relu\", \"tanh\", \"sigmoid\"])))\n",
    "    model.add(Dense(hp.Int(\"units_layer_4\", 200, 400, 50), activation=hp.Choice(\"activation_layer_4\", [\"relu\", \"tanh\", \"sigmoid\"])))\n",
    "    model.add(Dense(hp.Int(\"units_layer_5\", 50, 100, 50), activation=hp.Choice(\"activation_layer_5\", [\"relu\", \"softmax\"])))\n",
    "    \n",
    "    if hp.Boolean(\"add_layer_6\"):\n",
    "        model.add(Dense(hp.Int(\"units_layer_6\", 20, 50, 10), activation=\"softmax\"))\n",
    "        model.add(Dropout(hp.Float(\"dropout_2\", 0.0, 0.5, 0.1)))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")), loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "The Bayesian optimization method is used for hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from keras_tuner_dir\\bayesian_tuning_example\\tuner0.json\n",
      "Best hyperparameters: {'units_layer_1': 850, 'activation_layer_1': 'relu', 'add_layer_2': True, 'units_layer_3': 450, 'activation_layer_3': 'tanh', 'units_layer_4': 400, 'activation_layer_4': 'tanh', 'units_layer_5': 50, 'activation_layer_5': 'relu', 'add_layer_6': False, 'learning_rate': 0.008117079378823321, 'units_layer_2': 600, 'activation_layer_2': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tuner = kt.BayesianOptimization(build_model, objective=\"val_mean_squared_error\", max_trials=30, directory=\"keras_tuner_dir\", project_name=\"bayesian_tuning_example\")\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=16)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training\n",
    "The best model is trained with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilasv\\myenv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\ilasv\\myenv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1580/1580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 14ms/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 2/2\n",
      "\u001b[1m1580/1580\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0157 - val_mean_squared_error: 0.0157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2212d599930>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", restore_best_weights=True)\n",
    "best_model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), batch_size=16, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "best_model.save(\"name.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "Predictions are made, and performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m790/790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inverse Scaling and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "RMSE: 98190.90279984119\n",
      "MAE:  73841.54079612285\n",
      "MAPE: 26.846540227542015 %\n",
      "SMAPE: 22.693285625104515 %\n",
      "R2: 0.5034783329649752\n",
      "Test:\n",
      "RMSE: 108994.42932119273\n",
      "MAE:  81782.51711428494\n",
      "MAPE: 30.153101689544354 %\n",
      "SMAPE: 25.151863202018692 %\n",
      "R2: 0.45777846464393546\n"
     ]
    }
   ],
   "source": [
    "def my_inverse_scaler(y: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    tmp = np.concatenate((y.reshape((-1, 1)), X), axis=1)\n",
    "    return scaler.inverse_transform(tmp)[:, 0]\n",
    "\n",
    "# Inverse scaling\n",
    "y_pred_train = my_inverse_scaler(y_pred_train, X_train)\n",
    "y_train = my_inverse_scaler(y_train, X_train)\n",
    "y_test = my_inverse_scaler(y_test, X_test)\n",
    "y_pred = my_inverse_scaler(y_pred, X_test)\n",
    "\n",
    "# Accuracy evaluation\n",
    "print(\"Train:\")\n",
    "accuracy(y_train, y_pred_train)\n",
    "print(\"Test:\")\n",
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we built a neural network model to predict real estate prices based on historical data. After preprocessing the data, scaling it, and tuning the model's settings, we were able to make accurate predictions using all the available features. Interestingly, using only a few features that were somewhat related to the target (as identified by SHAP) did not improve the results. In fact, using all the features gave better performance. The model showed strong generalization on both the training and test sets, and early stopping helped prevent overfitting. Overall, the model highlights the potential of neural networks for predicting real estate prices and can be improved with more advanced features or optimization techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
